1. 创新点改为被动
2. 对首次出现的缩写进行说明

%%
%% Copyright 2007-2024 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version. The latest version of this license is in
%% http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%

% \documentclass[preprint,12pt]{elsarticle}
\documentclass[preprint,5p,times,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

% my
\usepackage{threeparttable,booktabs,multirow,subfigure,ulem,threeparttable,float}
% 三线表 程序伪代码
\usepackage[linesnumbered, ruled]{algorithm2e}
\SetKwRepeat{Do}{do}{while}%
\usepackage{ulem}
% 解决问题：Latex参考文献乱序
\usepackage{natbib}
\setcitestyle{sort&compress}
\bibliographystyle{unsrt}

\journal{Nuclear Physics B}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%% addressline={},
%% city={},
%% postcode={},
%% state={},
%% country={}}
%% \fntext[label3]{}

\title{Central Arterial Pressure Reconstruction Based on Cross-domain and Cross-modal Transfer Learning}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%% addressline={},
%% city={},
%% postcode={},
%% state={},
%% country={}}
%%
%% \affiliation[label2]{organization={},
%% addressline={},
%% city={},
%% postcode={},
%% state={},
%% country={}}

\author{} %% Author name

%% Author affiliation
\affiliation{organization={},%Department and Organization
			addressline={},
			city={},
			postcode={},
			state={},
			country={}}

%% Abstract
\begin{abstract}
%% Text of abstract
Central Arterial Pressure (CAP) is a critical indicator of cardiovascular health. Although deep learning methods have enhanced CAP reconstruction, they often require a substantial volume of invasive data for training. The acquisition of such datasets is impeded by high costs and prolonged durations, which restrict the improvements in accuracy of deep learning techniques. This paper introduces a novel approach, employing a cross-domain (simulated domain-real domain) and cross-modal (Photoplethysmogram-Radial Artery Pressure, PPG-RAP) pretraining paradigm, along with a new hybrid deep learning model, CBiMSA, for CAP reconstruction. The methodology incorporates cross-domain transfer learning using a virtual dataset from a 55-segment human arterial tree model and the Nektar1D hemodynamic dataset to pretrain the CBiMSA models. Additionally, a cross-modal transfer training approach is integrated, facilitating the reconstruction from PPG signals to Radial Artery Pressure (RAP) signals. These three pretrained models are then fine-tuned and validated on a real dataset comprising 62 individuals. The results demonstrate significant enhancements in the cross-domain or cross-modal transfer methods compared to eight commonly used deep learning models, with the cross-domain transfer learning based on the TLM simulation dataset yielding the best performance. Notably, our proposed model outperforms others in both transfer and non-transfer scenarios. In conclusion, cross-domain and cross-modal transfer learning for CAP reconstruction presents a method with considerable potential, effectively bridging the gap between cardiovascular hemodynamic simulation modeling and clinical physiological signal detection methods.

\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
%\includegraphics{grabs}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Central Arterial Pressure \sep Transfer Learning \sep Cross-domain, Cross-modal \sep Deep Learning
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

%% Use \section commands to start a section
\section{Introduction}
\label{sec:introduction}
Central arterial blood pressure (CAP) is the blood pressure of the aorta near the heart, and unlike peripheral arterial blood pressure, which can directly reflect the physiological and pathological state of the aorta, and has a stronger correlation with end-of-organ damage \cite{RN4}. CAP waveforms allow for a wealth of physiological information, such as impulse pressures, diastolic blood pressures (DBP) and systolic blood pressures (SBP), augmentation index and ejection time \cite{RN7}. CAP is an important risk factor for cardiovascular disease and can predict cardiovascular events more accurately than peripheral arterial pressure \cite{RN9, RN10}. Therefore, it is important to detect CAP waveforms accurately and in a timely manner in order to detect blood pressure abnormalities and reduce or prevent cardiovascular events \cite{RN8}.

The gold standard method of measuring CAP is indeed invasive and usually involves cardiac catheterization. This method, although highly accurate, can be painful for the patient, increases the risk of infection, and is not suitable for routine screening because of its invasive nature \cite{RN12}. Therefore, it is clinically important to develop a noninvasive CAP estimation method to facilitate routine clinical screening. Reconstruction of CAP waveforms from RAP waveforms has become a hot research topic because of its low cost, low risk, and simple operation.

Traditional methods, machine learning (ML), and deep learning (DL) method are applied in the reconstruction of CAP. Traditional methods such as autoregressive exogenous (ARX) \cite{chen1997estimation} and N-point moving average method (NPMA) \cite{RN55} have the advantage of being well-established and interpretable. However, they may lack accuracy in capturing complex non-linear relationships and require manual feature engineering \cite{xiao2022reconstruction}. With the progression of signal processing technology and the rise of machine learning (ML), ML for non-invasive blood pressure measurement has evolved into a prominent research area \cite{RN28,RN31,RN30,RN29}. While these ML techniques have shown promising results, the intricate process of manual feature extraction has posed a challenge to the final outcomes. Deep learning, as a novel ML domain, has also demonstrated advancements in blood pressure signal detection owing to its end-to-end training methodology \cite{RN32, RN33, RN34}. The collection and construction of massive datasets are crucial for enhancing the performance of deep learning models. However, the acquisition cost and long cycle of invasive datasets make it challenging to meet the requirements for constructing large-scale datasets. Therefore, exploring alternative methods for building massive datasets is of great value.

Simulation datasets generated by hemodynamic modeling are widely used in cardiovascular research to study the physiology and pathology of blood flow in the cardiovascular system. These datasets are essential for understanding the hemodynamic properties of the cardiovascular system, diagnosing cardiovascular conditions, and planning clinical treatments. Researchers have developed multimodal cardiovascular models that integrate cardiac electrophysiology with hemodynamic modeling, providing a broader understanding of the cardiovascular system \cite{roy2021multimodal}. There are different types of hemodynamic models, including 0D, 1D, and 3D models. 0D models \cite{Li_Wang_Mao_Liu_2019, Duanmu_Yin_Fan_Yang_Luo_2018} are simplified models that represent the entire cardiovascular system as a single compartment. 1D models \cite{RN52,RN43} are more complex and represent the cardiovascular system as a network of interconnected vessels. 3D models \cite{liu2016three, kim2010patient} are the most complex and represent the cardiovascular system as a detailed anatomical structure. Each model has its own advantages and disadvantages. 0D models are simple and computationally efficient but lack anatomical detail. 3D models are the most anatomically accurate but are computationally expensive and require a lot of data \cite{huygh2016hemodynamic,campos2012techniques}. Due to the computational efficiency and the capability to accurately represent wave propagation phenomena and simulate pressure and flow waveforms at any point in the arterial network, the 1D models actually possess the ability to construct large-scale datasets. In this study, we utilized two 1D hemodynamic models: the 55-segment human arterial tree transmission line model and the hemodynamic Nektar1D model.

Whether or not simulation datasets are positive for CAP reconstruction studies is an urgent need for research. In addition, exploring deep learning feature representations based on other physiological signal modalities is also worth investigating for CAP reconstruction.Therefore, in this paper, simulation datasets and noninvasive real cross-modal datasets are used to assist the training of DL models for CAP reconstruction, so as to realize high-precision CAP reconstruction. The main contribution of this paper:

\begin{itemize}
\item[$\bullet$] A novel cross-domain transfer learning method is proposed, utilizing a massive dataset generated by a hemodynamic simulation model for pretraining. Through this method, hemodynamic knowledge is embedded and waveform features are learned, enabling high-precision CAP reconstruction.

\item[$\bullet$] A novel cross-modal transfer learning method is proposed, which facilitates deep learning models in extracting universal feature representations from other physiological signal modalities. It is validated that non-invasive real datasets can be utilized as cross-modal training source datasets for CAP reconstruction.

\item[$\bullet$] A comprehensive comparison of the CbiMSA\_trans model has been conducted, including comparisons between different models, cross-domain and cross-modal comparisons, as well as comparisons between different simulated datasets. The results indicate that the CbiMSA\_trans model exhibits good performance in cross-modal transfer and across different simulated datasets, particularly excelling in the Dataset\_TLM dataset. This provides new potential value for research and applications in the field of cross-domain and cross-modal transfer of CAP.

\end{itemize}

\section{Related Works}
\subsection{Traditional methods}
The gold standard method for measuring CAP is indeed invasive and typically involves cardiac catheterization. This method, while highly accurate, can cause patient distress, increase the risk of infection, and is not suitable for routine screening due to its invasive nature \cite{RN12}. Therefore, the development of a non-invasive CAP estimation method to facilitate routine clinical examination is of clinical significance. The reconstruction of the CAP waveform from PBP waveforms is a research hotspot due to its low cost, low risk, and simple operation. Several methods have been proposed, including the direct alternative method of carotid artery endometry, the generalized transfer function(GTF) method \cite{RN14,RN15,RN16,RN13}, and the multichannel blind system identification(MBSI) algorithm \cite{RN21,RN20,RN19}. However, the MBSI algorithmic method requires the simultaneous acquisition of several different peripheral pressure waveforms \cite{RN16} and is extremely sensitive to noise, when the peripheral arterial pressure signal contains noise with a low signal-to-noise ratio \cite{RN18}. One of the main disadvantages of the GTF method is that it is based on population averages and may not adapt well to individual variations in pulse pressure (PP) amplification, which is the ratio of radial to central PP \cite{RN22}. This means that the GTF may not provide accurate results for all individuals, particularly those with conditions that affect blood pressure, such as elderly and hypertensive patients \cite{RN22}.

\subsection{Machine Learning and Deep Learning}
ML and DL methods for noninvasive blood pressure measurement are gradually becoming a hot research topic due to the advancement of signal processing techniques and the emergence of ML and DL. Many ML techniques have been recently established to estimate blood pressure (BP), such as support vector machine \cite{RN24}, random forest \cite{RN25}, and neural networks \cite{RN26,RN27}. Estimation of continuous SBP and DBP using linear regression ML models \cite{RN28,RN31,RN30,RN29}. However, there are still challenges in establishing better regression models for BP estimation due to limited feature selection in previous methods. Reconstruction of CAP using DL methods has shown promising results in recent studies \cite{liu2023central}. One such method, called Arterial-Net, uses a custom pretrained backbone and a tailored optimization function to address the challenges of arterial blood pressure (ABP) reconstruction under remote health settings. This method demonstrates superior performance in ABP waveform reconstruction and accurate systolic and diastolic blood pressure (SBP/DBP) estimations while significantly reducing subject variance \cite{RN32}. Another study proposed a hybrid DL model \cite{RN33} that utilized both raw signals and physical characteristics (age, height, weight, and gender) for non-invasive cuff-less blood pressure estimation. Furthermore, CBI-SAN \cite{RN34} model establishes an end-to-end relationship from the RAP waveform to the CAP waveform. It consists of a Convolutional Neural Network (CNN), a bidirectional Long Short-Term Memory network (Bi-LSTM), and a self-attention mechanism. These components work together to improve the performance of CAP reconstruction. However, both ML methods and DL methods face the problems of inability to guarantee the diversity of samples for invasive data measurements and the long periodicity of data acquisition. Simulation datasets generated by hemodynamic modeling, on the other hand, can effectively solve the problem.

\subsection{Hydrodynamic Models}
Simulation datasets generated by hemodynamic model are widely used in cardiovascular research to study the physiology and pathology of blood flow in the cardiovascular system \cite{RN36}. Hydrodynamic models and grid models are indeed two common approaches used to simulate hemodynamics, each with their own strengths and limitations. Hydrodynamic models, such as those developed using the finite element method, focus on local hemodynamic characteristics of arterial stenosis, including pressure and velocity fields in the proximal and distal regions of the arterial stenosis \cite{RN37, RN38}. These models can provide detailed insights into the behavior of blood flow in major arteries, including changes in arterial hydraulic impedance during a cardiac cycle \cite{RN37}. However, these models can be computationally expensive, making them less suitable for modeling entire arterial trees \cite{RN40, RN39}. On the other hand, the transmission line model (TLM), an electrical network model, has been used to overcome some of these limitations. The TLM was first proposed by McDonald and Taylor for human arterial trees, and various TLMs have since been developed and applied to the analysis of human arterial trees and hemodynamics in the context of pulse wave propagation \cite{RN41}. One-dimensional (1-D) and three-dimensional (3-D) computational hemodynamics models have also been used extensively to simulate arterial hemodynamics. These models can provide localized hemodynamic quantities such as wall shear stress and particle residence time, and can simulate complex processes such as the interactions between the arterial wall and medical devices \cite{RN39}. However, 1-D models require additional empirical laws to account for recirculation and pressure losses in the presence of vessel curvatures, stenoses, aneurysms, etc., which are intrinsically captured with 3-D models \cite{RN39}.

\subsection{Transfer Learning}
Transfer learning is a machine learning method that involves reusing a pretrained model as the starting point for a model on a new task. It is a popular approach in deep learning, particularly in computer vision and natural language processing tasks, as it enables the training of deep neural networks with less data compared to training from scratch. The key idea is to transfer knowledge from one or more source tasks to improve learning in a related target task. This approach is beneficial when there isn't enough labeled training data, or when a pretrained model on a similar task is available. By applying transfer learning, one can achieve significantly higher performance on a new task than training with only a small amount of data. The process involves repurposing a model trained on one task for a second, related task, allowing for rapid progress in modeling the second task.The related work on transfer learning for CAP reconstruction involves various techniques and models. Researchers have proposed methods to reconstruct the CAP waveform using transfer learning approaches. For instance, a study presented a technique to reconstruct the aortic pressure waveform from noninvasive acquisition of aortic blood flow velocity waveform using a transfer-function-free technique \cite{giudici2021transfer}. Additionally, a model topology consisting of three cascaded stages, including a blood pressure estimation stage, was proposed, which utilized transfer learning for personalized blood pressure estimation using photoplethysmography \cite{ali2022lstm}. Furthermore, transfer learning has been found effective for deep learning-based electrocardiogram (ECG) analysis, enabling the development of diverse and robust deep-learning models, particularly when researchers lack large ECG datasets \cite{jang2021effectiveness}.

These studies demonstrate the potential of transfer learning in the reconstruction and estimation of CAP waveforms, offering insights into the application of transfer learning techniques in the field of cardiovascular health monitoring and diagnosis.

\section{Method}
In this section, we first describe the source dataset Dataset\_PPG2RAP \cite{RN42}, Dataset\_TLM \cite{RN45}, Dataset\_HN \cite{RN43} and the target dataset RAP2CAP \cite{RN57}, the physiological metrics of the dataset are shown in Table \ref{t1}, and then we introduce the network architecture, the transfer learning technique and the loss function and evaluation metrics.

\begin{table*}[ht]
	\centering
	\caption{Statistics on systolic, diastolic and mean blood pressure for the source datasets Dataset\_HN, Dataset\_TLM, Dataset\_PPG2RAP and the real datasets Dataset\_Target}
	\label{t1}
	\renewcommand{\arraystretch}{1.5}
	\resizebox{\textwidth}{!}{ % 调整表格宽度
		\begin{tabular}{p{45pt}ccccccc}
		\toprule
		\multirow{2}{*}{Dataset} & \multirow{2}{*}{Sample Size} & \multicolumn{3}{c}{RAP(mean$\pm$std)} & \multicolumn{3}{c}{CAP(mean$\pm$std)} \\ \cline{3-8}
								   & & SBP & DBP & MAP & SBP & DBP & MAP \\
		\midrule
		Dataset\_HN & 3325 & 119.22±19.71 & 59.27±12.88 & 89.24±34.29 & 116.77±20.03 & 61.90±13.31 & 89.33±32.28 \\
		Dataset\_TLM & 4000 & 125±20.33 & 79.4±18.38 & 102.40±16.21 & 121.62±18.23 & 58.31±12.34 & 85.65±26.11 \\
		Dataset\_PPG2RAP$^{\mathrm{a}}$ & 12000 & 134.19±22.93 & 66.14±11.45 & 90.78±14.15 & 134.19±22.93 & 66.14±11.45 & 90.78±14.15 \\
		Dataset\_Target & 132 & 113.77±17.17 & 53.84±10.87 & 83.80±33.23 & 99.18±16.60 & 54.17±11.37 & 76.67±26.63 \\
		\bottomrule
		\multicolumn{8}{l}{$^{\mathrm{a}}$ Dataset\_PPG2RAP is the invasive acquisition dataset, using the cross-modal pulse wave signals PPG and RAP}
		\end{tabular}
	}
\end{table*}

\begin{table}[!t]
\caption{Clinical characteristics of 62 patients}
\label{t2}
\centering
\renewcommand{\arraystretch}{1.5}
	\begin{threeparttable} % 脚标注释
		\begin{tabular}{p{5cm}p{3cm}}
			\toprule
			Parameters & Mean ± SD\\
			\midrule
			Age, y & 61 $\pm$ 11 \\
			Male/female, n & 45/17 \\
			Previous hypertension ($\geq$ 140/90 mmHg) & 58 \\
			CAD/aortic stenosis/ASD, n & 60/1/1 \\
			Weight, kg & 83 $\pm$ 15 \\
			Height, cm & 172 $\pm$ 8 \\
			LVEF, \% & 47 $\pm$ 10 \\
			Cardiac irregularity, n & 3(AF=2,VEBs=1) \\
			\bottomrule
		\end{tabular}
		% \begin{tablenotes} % 注释
		% \footnotesize
		% SD: standard deviation; CAD: coronary artery disease; ASD: atrial septal defect; LVEF: left ventricular ejection fraction; AF: atrial fibrillation; VEBs: premature ventricular beats
		% \end{tablenotes}
	\end{threeparttable} % 脚标注释
\end{table}

\subsection{Data Acquisition and Preprocessing}
\subsubsection{Source Datasets}
\paragraph{Dataset\_TLM}
In order to validate and evaluate the effect of transfer learning on CAP reconstruction, the multi-scale virtual database used in this study. The database uses a human arterial tree model, and the original physiological data of the arterial tree was compiled by Xiao et al. \cite{RN51} and subsequently modified by Xiao et al. \cite{RN52}. The arterial model for this database is based on Noordergraaf's 55-segment model for representing the entire arterial system. The detailed vessel dimensions and elastic constants of the arterial model can be obtained from our previous studies \cite{RN51,RN55,RN53,RN52}.

\paragraph{Dataset\_HN}
To further validate the accuracy of the transfer learning model, the publicly available analog pulse wave signal dataset Nektar1D was used. The Nektar1D dataset is a virtual database created using a nonlinear one-dimensional model of human systemic circulating aortic blood flow. This model was used to generate a database of 3,325 virtual healthy adult subjects whose cardiac and arterial parameters varied over a physiologically healthy range, more detailed information can be found in Willemet et al. \cite{RN56}

\paragraph{Dataset\_PPG2RAP}
% To develop and evaluate the effects of cross-modal transfer learning for CAP reconstruction, the Multiparameter Intelligent Monitoring in Intensive Care(MIMIC-III) dataset from PhysioNet \cite{RN46} has been used. This database contains simultaneous PPG and ABP signals. The sampling rate for all signals is 125 Hz, recorded with 8-bit precision. In this work, we utilized the data compiled from MIMIC-III by Kachuee et al. \cite{RN31, RN48}. This compiled dataset is present in the University of California Irvine (UCI) ML Repository \cite{RN49}. The data present in this repository is in a convenient form to analyze, and the raw signals are already pre-processed followed by their algorithm. For the sake of convenience, ignoring the signal episodes with too tiny or too large blood pressure values and only considering signals with (60 mmHg $\leq$ DBP $\leq$ 130 mmHg) and (80 mmHg $\leq$ SBP $\leq$ 180 mmHg). In this study, we wanted to test our method on a broader range of signals since a real-world application scenario might exhibit extremely small and high BP values. Therefore, we considered signals with DBP as low as 50 mmHg and SBP as high as 200 mmHg. The statistics of the dataset are presented in Table \ref{t1}. It can be observed that SBP has a comparatively greater value of standard deviation.

To examine the impact of cross-modal transfer learning on CAP reconstruction, we utilized the MIMIC-III dataset from PhysioNet \cite{RN46}, which includes simultaneous PPG and ABP signals. These signals were all recorded at a sampling rate of 125 Hz with 8-bit precision. The dataset used in this study was compiled by Kachuee et al. \cite{RN31, RN48} and is available in the University of California Irvine (UCI) ML Repository. The data in this repository is well-prepared for analysis, with pre-processed raw signals. For the analysis, only signals within the range of 60 mmHg $\leq$ DBP $\leq$ 130 mmHg and 80 mmHg $\leq$ SBP $\leq$ 180 mmHg were considered, excluding episodes with extreme blood pressure values. However, to ensure the method's robustness in real-world scenarios with varying blood pressure values, signals with DBP as low as 50 mmHg and SBP as high as 200 mmHg were included in the study. The dataset's statistics are detailed in Table \ref{t1}, showing a higher standard deviation for SBP.

\subsubsection{Target Dataset}
\begin{figure}[t]
\centering %图片全局居中
\includegraphics[width=0.5\textwidth]{img/dataset/target.pdf}
\caption{Distribution of private database blood pressure values. SBP of the radial artery (RASP); DBP of the radial artery (RADP); SBP of the aortic root (CASP); DBP of the aortic root (CADP)}
\label{f1}
\end{figure}

The target dataset data were invasive blood pressure data recorded in a previous study by Pauca et al. \cite{RN57}. The study was approved by the Clinical Research Practice Committee and all study participants. The biological characteristics of the participants are shown in Table \ref{t2}. Invasive BP data recorded using the SphygmoCor CVMS (PWV Medical Sydney) device included high-fidelity BP from normal and diagnostic catheter measurements of CAP and RAP in 62 patients. Specifically, in this study, the response frequency of the manometry system was set to less than 20 Hz to obtain high-fidelity recordings of arterial pressure. To minimize data redundancy, the 200 Hz sampled arterial blood pressure data were resampled by linear difference at 128 Hz. In each set of samples, one CAP waveform and one RAP waveform were extracted. Random cuts of waveforms containing 1200-2000 samples were used for training and testing the reconstruction of CAP waveforms from RAP. The BP histogram of the target data is shown in Fig. \ref{f1}.

\subsubsection{Data Pre-preocessing}
During the blood pressure data collection process, abnormal data may arise due to the instability of blood pressure measurements. The Continuous Arterial Pressure (CAP) and Radial Arterial Pressure (RAP) values exhibited fluctuations within the ranges of 40-120 mmHg and 50-140 mmHg, respectively. To address the impact of outlier data points and the slow convergence of model training resulting from the wide range of blood pressure values, the Z-score method was employed for data standardization. Through this preprocessing step, the data was normalized to have a mean of 0 and a standard deviation of 1. This standardization technique enhanced the stability of training, facilitating the model to converge more efficiently towards the optimal solution. The formula for the Z-score method is presented below:

\begin{equation}x^{*}=\frac{x-\mu}{\sigma}\label{eq1}\end{equation}

In the formula provided, $x$ represents the original blood pressure value for either CAP or RAP. The parameters $\mu$ and $\sigma$ denote the mean and standard deviation, respectively, of the blood pressure waveform sample data within the segment. The normalized blood pressure value at the sampling point, denoted as $x^{*}$, is derived through this normalization process. In order to mitigate the impact of abnormal blood pressure values that may arise during sampling, this study employed thresholds of 20 mmHg and 180 mmHg as the minimum and maximum limits. Data falling outside these thresholds, indicating excessively low or high blood pressure values, were filtered out to uphold the data's reliability.

\subsection{Network Architecture}

\begin{figure*}[ht]
\centering %图片全局居中
\includegraphics[width=1\textwidth]{img/Model/CBiMSAxiu.drawio.pdf}
\caption{The CBiMSA network, serving as the pretraining network model, combines CNN, BiLSTM, and multi-head self-attention mechanism. CNN is employed to extract the potential local features of waveform signals, as the peaks and valleys of blood pressure effectively reflect the physiological state of the human body. Additionally, BiLSTM network is utilized to capture the global features of waveform signals and the long-term dependencies of cardiac contraction and relaxation, thus forming the periodic blood pressure waveform signals. Furthermore, the multi-head self-attention mechanism framework is used to focus more on exploring the dependencies in the data from multiple dimensions.}
\label{model}
\end{figure*}

CBiMSA is a DL model proposed by our team which does not require manual feature extraction as a traditional transfer function method. It combines CNN, BiLSTM and Multi-head Self-Attention Mechanism. CNN is used to mine the potential local features of waveform signals as the peaks and valleys of blood pressure can effectively reflect the physiology of the human body. Bi-LSTM network is used to mine the global features of waveform signals and long time-series dependencies as the heart systole and diastole form the periodically fluctuating waveform signals of blood pressure. We use the multiple self-attention mechanism framework to focus more on mining dependencies in the data from multiple dimensions. Since the back-and-forth fluctuations of blood pressure signals are not simple and regular, they are affected by various features and present different poses. In this study, the CBiMSA model was used to discover depth-level features of pulse signals and reconstruct CAP waveforms based on non-invasive RAP waveforms from in vivo data from 1,032 participants and in silico data from 4,374 healthy virtual subjects.

As shown in Fig. \ref{model}, the CBiMSA model is comprised of two distinct one-dimensional CNNs, a Bi-LSTM layer, a multi-head self-attention mechanism, and a fully connected neural network. Its structure can be segmented into three key modules: the input layer, the feature learning layer, and the waveform reconstruction layer. In this model, the RAP blood pressure signal serves as the input data, while the CAP blood pressure signal is the desired output. Specifically, the RAP signal $x_i=(x_1,x_2,x_3,\ldots,x_n)$ is fed into two separate parallel one-dimensional convolutional units conv, each with dimensions of 16 and 21. the number of convolutional kernels is 5, and the padding operation is applied to keep the sequence length constant. The processing of the two sets of waveform features involves the utilization of batch normalization and the ReLU activation functions, consecutively. This method not only realizes the nonlinear transformation of the blood pressure signal features, but also prevents the gradient vanishing and exploding, and speeds up the convergence of the network model training. The connection function is then used to overlay the processed data on the channel dimension. The waveform features $x_i^{'}=(x_1^{'},x_2^{'},x_3^{'},\ldots,x_n^{'})$ are obtained, and the process culminates in the learning of the local features. the BiLSTM is connected behind the CNN. all the features processed by the CNN are sent to the BiLSTM, which has 45 LSTM units. Global features extracted from the blood pressure waveforms are acquired through the distinctive gate structure inherent in the LSTM cells. Subsequently, the weight information within the hidden layer undergoes adjustment across multiple dimensions through a network of multi-head self-attention mechanisms. The resulting feature tensor information $y_i=(y_1,y_2,y_3,\ldots,y_n)$ is then fed into the fully connected layer for nonlinear transformation, ultimately yielding the final predicted CAP waveform data. Throughout this process, the multi-head self-attention mechanism plays a crucial role in enhancing the model's capacity to grasp both global and local features. The specific parameters of the CBiMSA model are detailed in Table \ref{para}.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{table*}[!t]
\centering
\caption{The detailed network structure of CBiMSA}
\label{para}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cccc}
\toprule
Structure & layer & Parameter & Activation function \\ \midrule
\multirow{5}{*}{Feature extractor} & Conv1 & 16, 5 & Relu \\
								   & Conv2 & 21, 5 & Relu \\
								   & BiLSTM & 45 & - \\
								   & SeqSelfAttention\_1 & 0.0001$^{\mathrm{a}}$ & sigmoid \\
								   & SeqSelfAttention\_2 & 0.0001$^{\mathrm{a}}$ & sigmoid \\ \midrule
Feature predictor & FC & 1 & - \\ \bottomrule
\multicolumn{4}{l}{$^{\mathrm{a}}$attention regularizer weight}
\end{tabular}
\end{table*}

\subsubsection{BiLSTM Block}
As a special kind of RNN, LSTM contains three gates in the internal structure of the network, i.e., forgetting gate, input gate and output gate. The three gated storage units control which information should be forgotten or retained 57. In the input blood pressure signal time series, $X = (x_1, X_2, X3, \ldots ,x_n)$, $X \in N$ and $N$ is the total number of sampling points for each blood pressure signal 500. The forgetting gate is tasked with acting upon the cell state $C_{(t-1)}$. Its objective is to selectively erase or retain information regarding the unit state, making decisions on what aspects should be discarded or preserved. Similarly, the input gate also operates on the cell state $C_{(t-1)}$ by determining the new information to be stored within the cell state. Meanwhile, the output gate's action influences the final output $h_{(t-1)}$ of the LSTM cell. Within a single cell, the inputs include the cell state $C_{(t-1)}$ from the previous time step, the hidden layer $h_t$, and the current time step's input $x_t$, which collectively contribute to the creation of a new cell state $C_t$ and hidden layer $h_t$. The computation of these components is determined by the following process.

\begin{align}
	\begin{split}
	C_t = \sigma(W_f\cdot[h_{t-1},x_t]+b_f)\cdot C_{t-1} + \sigma (W_i\cdot \\
	[h_{t-1},x_t]+b_t)\cdot tanh(W_c \cdot[h_{t-1},x_t]+b_c)
	\end{split}
\end{align}
\begin{align}
	h_t = \sigma(W_o\cdot[h_{t-1},x_t]+b_o)\cdot tanh(C_t)
\end{align}

where W represents the weight of the hidden layer, and b is the bias vector associated with the corresponding hidden layer. The signal at time step $t$, denoted as $h_t$, is computed using the activation functions $f$ and $g$ respectively. Given the unique internal configuration of a single LSTM cell, information about future time steps remains unknown. Nevertheless, each time step of the blood pressure signal exhibits close ties to its contextual surroundings. To address the issue of dependency on both preceding and subsequent information, a network can be constructed utilizing a two-layer structure with bidirectional connections.

\subsubsection{Multi Head Self Attention}
While LSTM has advantages over traditional models in sequence modeling by enabling long-term memory functions, it may still face limitations in terms of memory capacity. To enhance the network model's ability to learn from bidirectional information in front-back dependent time series, the self-attention mechanism is employed. This mechanism aids in searching for crucial internal information, addressing issues of gradient vanishing and explosion during training on long sequences. Unlike LSTM, the self-attention mechanism does not discard relatively unimportant information but rather redistributes weights to prioritize essential information, facilitating a better understanding of data interrelations. By weighting and aggregating data and weight information, the model is trained more effectively. The application of the attention mechanism has shown significant benefits in signal and image processing, enhancing model training outcomes. The specific process involves the utilization of the attention mechanism to improve the model's understanding of both past and future time series data.The specific process is as follows:

The self-attention model maps each sequence $X=(x_1,x_2,x_3,\ldots,x_n), X\in{R^{D_x\times N}}$ obtained from the BiLSTM network to three different spaces to obtain three vectors: the query vector $q_i\in{R^{D_x}}$, the key vector $k_i\in{R^{D_x}}$ and the value vector.

\begin{align}
	Q&=X\cdot W_q\in R^{D_x\times N} \\
	K&=X\cdot W_k\in R^{D_x\times N} \\
	V&=X\cdot W_v\in R^{D_x\times N}
\end{align}

$W_q, W_k$ and $W_v$ are the parameter matrices of the linear mapping, $Q=(q_1,q_2,\ldots,q_N)$, $K=(k_1,k_2,\ldots,k_N)$ and $V=(v_1,v_2,\ldots,v_N)$, which are matrices consisting of the query vectors, the key vectors and the value vectors, respectively.

\subsection{Transfer Learning}
\begin{figure*}[t]
\centering %图片全局居中
\includegraphics[width=1\textwidth]{img/Model/Transfer.pdf}
\caption{A transfer learning method, CBiMSA\_trans, is proposed. The CBiMSA\_trans model first undergoes pretraining using a rich hemodynamic virtual dataset, which includes cross-domain datasets Dataset\_TLM, Dataset\_HN, and cross-modal dataset Dataset\_PPG2RAP. Subsequently, fine-tuning of the convolutional layers and fully connected layers is conducted using the target real dataset.}
\label{transfer}
\end{figure*}

Considering the traumatic data measurement sample diversity of the CAP signal cannot be guaranteed and the long periodicality of data acquisition, and the existence of more publicly available virtual datasets of physiological signals on the Internet, which can therefore be centered around a large number of virtual datasets, we introduce a transfer learning method to reconstruct the CAP signal. By training DL models on virtual datasets and fine-tuning them on real data, the transfer learning approach is utilized so that the models can be better adapted to the target data. This approach can use the statistical information and features of the virtual data to reconstruct the target data, which further improves the prediction accuracy and generalization ability of the model, and also reduces the dependence on the original CAP signals, thus improving the practicality of the model.

\begin{algorithm}[htbp]
	\KwIn{\;
		$D_{A}=\{(x_{1}^{A},y_{1}^{A}), (x_{2}^{A},y_{2}^{A}),…, (x_{n}^{A},y_{n}^{A})\}$: source dataset\;
		$D_{B}=\{(x_{1}^{B},y_{1}^{B}), (x_{2}^{B},y_{2}^{B}),…, (x_{n}^{B},y_{n}^{B})\}$: target dataset\;
		$N$ : total epochs;
		$n_b$ : batch size
	}
	% \textbf{Phase 1:}\;
	Train a base model on $D_{A}$ and return the extracted features and parameters $\theta_A$.\;
	Initialize attributes of target network with parameters $\theta_B$ obtained from step2.\;
	\For { $c = 1,2,…,N$ }{
		\For { $m = 1,2,…, \lfloor N_b/n_b \rfloor $ }{
			Calculate $L$ of $f(\mathbf{x_{i}^{B}},\theta_{B})$ and $f(\mathbf{x_{i}^{A}},\theta_{B})$ as Eq.(9) \;
			Backpropagation to update network parameters $\theta_B$ by optimizing $L$
		}
	}
	\KwOut{
		Parameters in trained network architecture
	}
	\caption{Transfer Learning Algorithms}
	\label{alg}
\end{algorithm}

Figure \ref{transfer} illustrates our approach, which initially entails the cross-domain and cross-modal transfer learning of the RAP and CAP generated in the virtual database, along with the PPG and RAP data collected in the real domain. In this process, deep learning (DL) methods are utilized to take RAP as input, while the CAP serves as the label, aiming to input RAP data and predict CAP data. This approach facilitates the training of a model capable of learning robust feature extractors from data across multiple individuals, which can be transferred between patients. Subsequently, transfer learning is employed to apply this model to the real dataset, specifically the target domain dataset.

For the CAP reconstruction task, a commonly used loss function is the Mean Absolute Error (MAE). MAE Ioss function calculates the absolute differences between the predicted and true values. In a transfer learning formulation, if we assume that L is the MSE loss function, $A$ denotes the source domain, $B$ denotes the target domain, $\theta_A$ denotes the source model parameter, $\theta_B$ denotes the target model parameter, $D_{A}$ denotes the source datasets, $D_{B}$ denotes the target datasets, and $x_i^{A/B}$ and $y_i^{A/B}$ respectively denote the inputs and outputs of the source/target domains, respectively. Then the specific formula for the MAE loss on the target domain Bis as follows:

\begin{align}
	L_1=\dfrac{1}{n_B}\sum_{i=1}^{n_B}\left|f(\mathrm{x}_\mathrm{i}^B,\theta_B)-y_i^B\right|
\end{align}

Here $f(\mathrm{x}_1^\mathrm{B},\theta_B)$ is the predicted value of the model for the $i^{th}$ sample in the target domain B, $y_i^B$ is the actual value, and $n_B$ is the number of samples in the target domain. The regularization term $D(\theta_R)$ is the L2 regularization with the formula shown:

\begin{align}
	L_{2}=\lambda\parallel\theta_{B}\parallel^{2}
\end{align}

Here $\|\theta_{B}\|^{2}$ denotes the square of the L2 norm of the $\theta_{B}$ parameter vector, i.e., the sum of all parameter squares, and $\lambda$ is the regularization factor. The MAE loss on the source domain A:

\begin{align}
	L_3=\alpha\sum_{i=1}^{n_A}|f(\mathrm{x}_\mathbf{i}^A,\theta_B)-y_i^A|
\end{align}

Here $f(\mathrm{x}_1^\Lambda,\theta_B)$ is the predicted value of the model on the $i$th sample in the souroe domain A, $y_i^A$ is the corresponding actual value, $n_A$ is the number of sarıples ir the source domain, and $\alpha$ is the weighting coefficient of this portion of the loss in the overall objective function. The complete transfer learning loss function formula is as follows:

\begin{align}
	\begin{split}
		F_{A\to B}(\theta_B) &= \operatorname{argmin}_{\theta_B}\left(\frac{1}{n_B}\sum_{i=1}^{n_B}\lvert f(\mathrm{x}_i^B,\theta_B)-y_i^B\rvert \right.\\
		&\quad +\lambda\parallel\theta_B\parallel^2+\alpha\sum_{i=1}^{n_A}\lvert f(\mathrm{x}_i^A,\theta_B)-y_i^A\rvert\left.\right)
	\end{split}
	\label{eqtrans}
\end{align}

where the first term in the equation is the loss function in the target domain, the second term is the regularization term, and the third term is the loss function in the source domain plus a factor $\alpha$. The $\lambda$ and $\alpha$ are hyperparameters that need to be tuned. It is important to note that usually in transfer learning, we only optimize the parameter $\theta_{B}$ in the target domain, while the parameter $\theta_A$ in the source domain is usually pretrained and kept constant or only fine-tuned during the transfer process. Therefore, only $\theta_B$ is optimized in \eqref{eqtrans}.

With a new training paradigm, transfer learning, we address data limitations in CAP estimation and improve the performance of the model by learning robust feature extractors and shared knowledge using deep and transfer learning methods. The detailed learning process of CBiMSA\_trans is presented in Algorithm \ref{alg}.

\subsection{Evaluation Metrics}
In this study, the MAE and Spearman correlation coefficient (SCC) were used to evaluate the effect of reconstructing the CAP waveform. In addition, physiologically important metrics such as CASCP and CADP were evaluated using RMSE and MAE.

\subsubsection{MAE}Mean Absolute Error is the mean absolute error between the reconstructed CAP waveform and the actual measured CAP waveform.
\begin{align}
MAE&=\frac1m\sum_{i=1}^m|h(x_i)-y_i|
\end{align}
where $h(x_i)$ takes the sample point. $y_i$ represents the actual measurements at $i$ CAP sampling points, and $m$ represents the total number of CAP sampling points in the current section.

\subsubsection{RMSE}Root Mean Square Error is the standard deviation of the error between the reconstructed CAP waveform and the actual measured CAP waveform.

\begin{align}
	RMSE&=\sqrt{\frac1m\sum_{i=1}^{m}(h(x_{i})-y_{i})^{2}}
\end{align}

where $h(x_i)$ represents the predicted value of the reconstructed $i$ CAP sampling point. $y_i$ represents the actual measured value of the $i$ CAP sampling point. $m$ represents the total number of CAP sampling points in the current segment. the smaller the values of MAE and RMSE, the better the waveform reconstruction effect. The smaller the MAE and RMSE values, the better the waveform reconstruction.

\subsubsection{SCC}
Spearman correlation coefficient evaluates the monotonic relationship between two continuous variables (predicted and true values). In a monotonic relationship, the variables tend to change together, but not necessarily at a constant rate.

\begin{align}
	\rho &= 1-\frac{6\cdot\sum_{i=1}^{n}d_{i}^{2}}{n\cdot(n^{2}-1)},(d_{i}=rank_{x_{i}}-rany_{y_{i}})
\end{align}

where $n$ refers to the number of sampling points of the CAP waveform, and $d$ refers to the positional difference between the predicted and ranked actual values, respectively. A Spearman's correlation coefficient greater than 0 is a positive correlation; a Spearman's correlation coefficient less than 0 is a negative correlation. The closer to 1 and -1, the stronger the correlation. A Spearman's correlation coefficient of 0 means that X and Y are not correlated.

\section{Results}
In this section, We outline the experimental configuration and juxtapose our CBiMSA estimation outcomes with and without transfer learning against those of previous methodologies. We investigate how the performance of different CAP reconstructions is affected by different hemodynamic virtual datasets and noninvasive real datasets, and demonstrate the optimality of our reconstruction results obtained by training with a transfer learning paradigm. We validate the consistency of our method with the gold standard reconstructed CAP method by Bland-Altman and correlation method analysis.

\subsection{Experiment Setting}
We implemented and evaluated our DL models using the Pytorch library in a python environment on an Intel Xeon Gold 6226R 2.9GHz sixteen-core and 32GB RAM computer. an Nvidia GeForce GPU was used for network training. The invasive private dataset was divided into training and test sets in a 9:1 manner. Since a small-scale dataset is used in this study, the cross-validation method can reduce overfitting to some extent and is more conducive to validating the network model. In addition, the number of raw data samples is limited. Underfitting can be accessed when training small data samples. Therefore, data augmentation methods are used to enable more data to be generated from limited data, increasing the number of training sets to improve the robustness of the model. Specifically, a sliding window approach was used. The original sample points for each sample were obtained from 1200 to 2000, and multiple waveforms were randomly truncated multiple times to obtain 500 sample points for data augmentation. The first 100 sample points at the end of the original waveform are removed during the random truncation process to prevent sampling anomalies in these two waveforms from affecting the model training results. Each of the newly intercepted waveforms contains multiple blood pressure cycles, and there is also an overlapping sampling point between each waveform. Due to the random nature of waveform interception, this method does not affect the feature extraction results.
\begin{table}[!t]
\centering
\caption{The detailed training parameters of CBiMSA}
\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{p{4cm}p{4cm}}
	\toprule
	Items & Parameters setting \\
	\midrule
	Prediction loss & Mean absolute error \\
	Total Epochs & 150 \\
	Batch Size & 64 \\
	Optimizer & Adam \\
	$\lambda$ & 10 \\
	Learning rate & {[}0.001,0.0001{]} \\
	Bias Regularizer & 0.0001 \\
	Kernel Regularizer & 0.0001 \\
	\bottomrule
	\end{tabular}
\end{table}

\begin{table*}[ht]
\caption{Comparison of the effects of different models without transfer learning. The best results are in bold and the second best results are underlined. maE: mean absolute error; RMSE: root mean square error; r: Pearson's correlation coefficient. $\uparrow$ indicates that larger values are better, while $\downarrow$ vice versa.}
\centering
\renewcommand{\arraystretch}{1.5}
	\setlength{\tabcolsep}{6mm}{
	\begin{tabular*}{\linewidth}{ccccccc}
	\toprule
	Net\_Name & MAE\textsubscript{CAP}$\downarrow$ & MAE\textsubscript{CASP}$\downarrow$ & RMSE\textsubscript{CASP}$\downarrow$ & MAE\textsubscript{CADP}$\downarrow$ & RMSE\textsubscript{CADP}$\downarrow$ & SCC\textsubscript{CAP}$\uparrow$ \\ \midrule
	GRU & 5.06±0.95 & 6.15±1.60 & 7.94±1.27 & 5.69±0.98 & 7.15±0.91 & 0.79 \\
	LSTM & 5.68±1.38 & 5.06±1.19 & 6.87±1.46 & 5.50±0.55 & 6.67±0.55 & 0.76 \\
	BiLSTM & 3.73±1.03 & 4.54±1.22 & 5.89±1.75 & 2.63±0.50 & 3.88±0.93 & 0.9 \\
	CNNLSTM & 3.11±1.00 & 3.93±1.20 & 5.49±1.42 & 2.80±0.50 & 4.41±0.73 & 0.93 \\
	CNNGRU & 3.55±0.95 & 6.08±0.91 & 7.49±1.06 & 4.23±0.48 & 5.69±0.43 & 0.91 \\
	CNNBiLSTM \cite{xiao2022reconstruction} & 3.24±1.09 & 4.28±0.90 & 5.63±1.20 & 3.02±0.35 & 4.24±0.77 & 0.92 \\
	CBiSAN \cite{xiao2023reconstruction} & 3.34±1.18 & 4.59±1.23 & 6.03±1.59 & 2.88±0.70 & 4.03±1.24 & 0.93 \\
	CBiMSA & \textbf{2.65±0.67} & \textbf{2.83±0.83} & \textbf{3.96±1.54} & \textbf{1.51±0.38} & \textbf{1.92±0.42} & \textbf{0.94} \\
	\bottomrule
	\end{tabular*}}
\end{table*}

In our non-transfer approach, DL models were trained on an invasive dataset consisting of 62 patients. Each model was specifically trained using data solely from this invasive dataset. As transfer learning was not employed, the initial model's parameters were randomly initialized, and all layers were updated during the training process. The models were trained using a learning rate of [0.01, 0.0001] and a batch size of 64.

For the transfer learning method, CBiMSA\_trans, the learning rate and batch size are initially set to [0.01,0.0001] and 256, respectively. With an increase in the quantity of training data, it is possible to reduce the learning rate and augment the batch size, thereby accommodating a greater number of update iterations within each epoch. During the fine-tuning process of the pretrained model for the target dataset, the learning rate is modified to fall within the range of 0.01 to 0.0001, while the batch size is set to 64. Only the specific layers mentioned in Section II (C) are updated during this process. To prevent overfitting, early stopping is implemented when errors on the validation set begin to increase, preserving the learned network weights.

\subsection{Original Learning Results (without transfer)}

We use other current DL methods to compare and analyze with the CBiMSA\_trans model to verify the effectiveness of different DL network models in reconstructing CAP, CASP, and CADP, and the results are shown in Table V. The CAP metrics reconstructed by CBiMSA\_trans have the smallest MAE\textsubscript{CAP} value (2.65±0.67) and the highest SCC CBiMSA\_trans value (0.94). For CASP detection, both MAE\textsubscript{CASP} value (2.83±0.83) and RMSE\textsubscript{CASP} value (3.96±1.54) were the smallest. As for CADP, the MAE\textsubscript{CADP} value (1.51±0.38) and the RMSE\textsubscript{CADP} value (1.92±0.42) are also the smallest. It can be seen that among many DL models, the CBiMSA\_trans model is the best in reconstructing the CAP waveform, and the important physiological metrics of CADP and CASP are also the best. In general, recurrent neural networks such as GRU, LSTM and BiLSTM alone are not as accurate as CNNGRU, CNNLSTM, CNNBiLSTM and other neural networks with convolutional operations. Meanwhile, the network model reconstruction using multi-head attention mechanism is better than single-head attention mechanism and inapplicable attention mechanism.

\begin{figure}[ht]
\centering %图片全局居中
\subfigure[CASP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.11}
\includegraphics[width=0.20\textwidth]{img/Results/Origin/regression/bf.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.22}
\includegraphics[width=0.20\textwidth]{img/Results/Origin/bland_altman/Mean CASP_CBi-SAN & CASP_Inv(mmHg).pdf}}
\subfigure[CADP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.33}
\includegraphics[width=0.20\textwidth]{img/Results/Origin/regression/bg.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.44}
\includegraphics[width=0.20\textwidth]{img/Results/Origin/bland_altman/Mean CADP_CBi-SAN & CADP_Inv(mmHg).pdf}}
\caption{Linear regression plots and bland altman plots of CAP systolic and diastolic blood pressure data in Dataset\_target without transfer learning.}
\label{Fig.main}
\end{figure}

To further validate the effects of CASP and CADP in the CAP waveform reconstructed by the CBiMSA\_trans model, we analyzed the CBiMSA\_trans method using linear regression plots and Bland-Altman plots, as depicted in Fig. 4. The Bland-Altman plots display the average between the CASP or CADP reconstructed by different models and the CASP or CADP of the invasive measurements on the x-axis, while their difference is shown on the y-axis. The solid red line represents the standard deviation of the prediction by a factor of 1.96, indicating the upper and lower limits of the 95\% consistency bounds, while the gray dashed line indicates the average of the prediction errors. The analysis reveals that the majority of the error values of CASP and CADP detected by each method fall within the 95\% consistency limit. Furthermore, the CASP and CADP error values detected by the CBiMSA\_trans DL method outside the 95\% consistency limit are relatively small, with a large number of error values and an average error value of about 0. Based on the visual analysis and metric results, the coefficient of determination r\textsuperscript{2}\textsubscript{CASP} of the CBiMSA\_trans method is 0.98, with an RMSE\textsubscript{CASP} value of 3.96. Similarly, the coefficient of determination r\textsuperscript{2}\textsubscript{CADP} is 0.99, with an RMSE\textsubscript{CADP} of 1.92.

\subsection{transfer Learning Results}

The performance of different DL models for reconstructing cardiovascular signals in three datasets Dataset\_PPG2RAP, Dataset\_HN and Dataset\_Targetm, which is shown in Table VI. These models include GRU\_trans, LSTM\_trans, BiLSTM\_trans, CNNLSTM\_trans, CNNGRU\_trans, CNNBiLSTM\_trans, CBiSAN\_trans, and CBiMSA\_trans. The results show that the performance of cardiovascular signal reconstruction in a source dataset of Dataset\_TLM, the CBiMSA\_trans model has the best performance in reconstructing CAP, with the lowest MAE\textsubscript{CAP} value (2.17±0.21) and the highest SCC\textsubscript{CAP} value (0.96). In CASP assay, both MAE\textsubscript{CASP} value (1.77 ± 0.32) and RMSE\textsubscript{CASP} value (2.30 ± 0.32) were the smallest. As for CADP, its MAE\textsubscript{CADP} value (1.22 ± 0.26) and RMSE\textsubscript{CADP} value (1.51 ± 0.30) were also minimal.

\begin{table*}[htbp]
\caption{Comparison of the effectiveness of different models of transfer learning. The best results are shown in bold, the second best results are underlined, and the third best results are shown in wavy lines.}\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{1mm}{
\begin{tabular*}{\linewidth}{cccccccc}
\toprule
Datasets & Method & MAE\textsubscript{CAP}$\downarrow$ & MAE\textsubscript{CASP}$\downarrow$ & RMSE\textsubscript{CASP}$\downarrow$ & MAE\textsubscript{CADP}$\downarrow$ & RMSE\textsubscript{CADP}$\downarrow$ & SCC\textsubscript{CAP}$\uparrow$ \\ \midrule
\multirow{8}{*}{Dataset\_PPG2RAP}
		& GRU\_trans & 4.62±2.26 & 5.79±1.72 & 7.66±1.58 & 5.13±1.04 & 6.82±1.43 & 0.83\\
		& LSTM\_trans & 4.31±0.79 & 4.44±1.00 & 6.25±1.19 & 3.74±0.42 & 5.48±0.39 & 0.85\\ & BiLSTM\_trans & 3.03±1.16 & 3.16±1.03 & 4.49±1.74 & 1.97±0.44 & 2.95±0.97 & 0.93 \\
		& CNNLSTM\_trans & 3.11±1.11 & 3.53±1.04 & 4.92±1.48 & 2.34±0.44 & 3.69±0.77 & 0.92 \\
		& CNNGRU\_trans & 3.37±1.15 & 6.22±1.03 & 7.63±1.21 & 3.89±0.67 & 5.59±0.46 & 0.92 \\
		& CNNBiLSTM\_trans & 2.95±1.13 & 3.53±1.02 & 5.02±1.59 & 2.50±0.41 & 3.83±1.17 & 0.93 \\
		& CBiSAN\_trans & 3.21±0.91 & 3.42±0.97 & 4.87±1.48 & 2.70±0.46 & 3.83±0.95 & 0.93 \\
		& CBiMSA\_trans & \uwave{2.28±0.40} & \uwave{2.12±0.35} & \uwave{2.88±0.45} & \uwave{1.73±0.30} & \uwave{2.41±0.38} & \underline{0.95} \\ \midrule
\multirow{9}{*}{Dataset\_HN}
		& GRU\_trans & 5.31±1.06 & 6.46±1.31 & 8.36±1.31 & 5.92±1.01 & 7.51±0.92 & 0.76 \\
		& LSTM\_trans & 5.98±1.46 & 5.37±1.55 & 7.27±1.54 & 5.74±0.55 & 7.00±0.54 & 0.74 \\
		& BiLSTM\_trans & 3.94±1.08 & 4.75±1.27 & 6.13±1.85 & 2.77±0.52 & 4.07±0.96 & 0.84 \\
		& CNNLSTM\_trans & 3.23±1.02 & 4.11±1.22 & 5.75±1.46 & 2.98±0.53 & 4.63±0.72 & 0.92 \\
		& CNNGRU\_trans & 3.72±1.01 & 6.32±0.94 & 7.89±1.17 & 4.41±0.54 & 5.97±0.43 & 0.89 \\
		& CNNBiLSTM\_trans & 3.45±1.14 & 4.43±0.95 & 5.93±1.22 & 3.12±0.35 & 4.45±0.84 & 0.93 \\
		& CBiSAN\_trans & 3.24±0.92 & 4.64±1.19 & 5.61±1.15 & 2.66±0.68 & 3.95±1.48 & 0.93 \\
		& CBiMSA\_trans & \underline{2.23±0.22} & \underline{1.85±0.19} & \underline{2.56±0.33} & \underline{1.54±0.24} & \underline{2.18±0.34} & \textbf{0.96} \\ \midrule
\multirow{8}{*}{Dataset\_TLM}
		& GRU\_trans & 4.65±2.36 & 5.55±1.36 & 7.65±0.94 & 5.38±1.16 & 7.15±0.82 & 0.82 \\
		& LSTM\_trans & 5.08±0.82 & 4.85±1.13 & 6.56±1.33 & 4.50±0.66 & 6.30±0.60 & 0.79 \\
		& BiLSTM\_trans & 3.35±1.07 & 3.34±1.04 & 4.82±1.56 & 3.18±0.42 & 4.23±0.64 & 0.92 \\
		& CNNLSTM\_trans & 3.11±1.03 & 3.32±0.85 & 4.73±1.34 & 2.23±0.39 & 3.43±0.78 & 0.93 \\
		& CNNGRU\_trans & 3.46±0.86 & 5.13±0.82 & 6.82±0.97 & 3.54±0.59 & 5.27±0.71 & 0.91 \\
		& CNNBiLSTM\_trans & 3.21±0.84 & 3.82±0.83 & 5.17±1.16 & 2.78±0.37 & 3.91±0.66 & 0.93 \\
		& CBiSAN\_trans & 3.22±0.87 & 3.33±1.06 & 4.77±1.49 & 2.29±0.25 & 3.30±0.63 & \uwave{0.94} \\
		& CBiMSA\_trans & \textbf{2.17±0.21} & \textbf{1.77±0.23} & \textbf{2.30±0.32} & \textbf{1.22±0.26} & \textbf{1.51±0.30} & \textbf{0.96} \\
		\bottomrule
\end{tabular*}
}
\end{table*}

\begin{figure*}[ht]
\centering %图片全局居中
\subfigure[CASP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.111}
\includegraphics[width=0.22\textwidth]{img/Results/TLM/regression/bf.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.222}
\includegraphics[width=0.22\textwidth]{img/Results/TLM/bland_altman/Mean CASP_CBi-SAN & CASP_Inv(mmHg).pdf}}
\subfigure[CADP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.333}
\includegraphics[width=0.22\textwidth]{img/Results/TLM/regression/bg.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.444}
\includegraphics[width=0.21\textwidth]{img/Results/TLM/bland_altman/Mean CADP_CBi-SAN & CADP_Inv(mmHg).pdf}}
\caption{Linear regression plots and bland altman plots of CAP systolic and diastolic blood pressure data after transfer learning in Dataset\_TLM}
\label{Fig.main1}
\end{figure*}

\begin{figure*}[ht]
\centering %图片全局居中
\subfigure[CASP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.19}
\includegraphics[width=0.22\textwidth]{img/Results/HN/regression/bf.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.29}
\includegraphics[width=0.22\textwidth]{img/Results/HN/bland_altman/Mean CASP_CBi-SAN & CASP_Inv(mmHg).pdf}}
\subfigure[CADP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.39}
\includegraphics[width=0.22\textwidth]{img/Results/HN/regression/bg.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.49}
\includegraphics[width=0.21\textwidth]{img/Results/HN/bland_altman/Mean CADP_CBi-SAN & CADP_Inv(mmHg).pdf}}
\caption{Linear regression plots and bland altman plots of CAP systolic and diastolic blood pressure data after transfer learning in Dataset\_HN}
\label{Fig.main9}
\end{figure*}

\begin{figure*}[htbp]
\centering %图片全局居中
\subfigure[CASP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.01}
\includegraphics[width=0.22\textwidth]{img/Results/PPG2RAP/regression/bf.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.02}
\includegraphics[width=0.22\textwidth]{img/Results/PPG2RAP/bland_altman/Mean CASP_CBi-SAN & CASP_Inv(mmHg).pdf}}
\subfigure[CADP(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
\label{Fig.sub.03}
\includegraphics[width=0.22\textwidth]{img/Results/PPG2RAP/regression/bg.pdf}}
\subfigure[Average(mmHg)]{
\rotatebox{90}{\scriptsize{~~~~~~Difference(mmHg)}}
\label{Fig.sub.04}
\includegraphics[width=0.21\textwidth]{img/Results/PPG2RAP/bland_altman/Mean CADP_CBi-SAN & CADP_Inv(mmHg).pdf}}
\caption{Linear regression plots and bland altman plots of CAP systolic and diastolic blood pressure data after transfer learning in Dataset\_PPG2RAP}
\label{Fig.main0}
\end{figure*}

% \begin{figure*}[ht]
% \centering %图片全局居中
% \subfigure[Time/sampling point]{
% \rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
% \label{Fig.sub.1}
% \includegraphics[width=0.20\textwidth]{img/Results/Reconstrucion/output.png}}
% \subfigure[Time/sampling point]{
% \rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
% \label{Fig.sub.2}
% \includegraphics[width=0.20\textwidth]{img/Results/Reconstrucion/output2.png}}
% \subfigure[Time/sampling point]{
% \rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
% \label{Fig.sub.3}
% \includegraphics[width=0.20\textwidth]{img/Results/Reconstrucion/output3.png}}
% \subfigure[Time/sampling point]{
% \rotatebox{90}{\scriptsize{~~~~~~Pressure(mmHg)}}
% \label{Fig.sub.4}
% \includegraphics[width=0.20\textwidth]{img/Results/Reconstrucion/output4.png}}

% \caption{A typical sample of CBP measured and estimated using the CBiMSA\_trans model, where the blue solid line is measured CAP and the red dashed line is predicted CAP.}
% \label{Fig.main00}
% \end{figure*}

These findings indicate that the CBiMSA\_trans model excels in reconstructing CAP waveforms and important physiological indicators of CADP and CASP. Under the new training paradigm of transfer learning, all DL models demonstrated improved performance on various source datasets. Furthermore, the transfer effect varied across different datasets, with the best transfer effect observed on the source dataset Dataset\_TLM, followed by Dataset\_HN, while Dataset\_PPG2RAP lagged behind the first two datasets. This variation in transfer effect can be attributed to factors such as the signal quality of the source dataset and whether it is a cross-modal dataset. It was observed that while the cross-modal dataset used as the source dataset for transfer learning significantly enhanced the CAP reconstruction effect, the reconstruction effect was not as robust as that of the virtual dataset generated by hemodynamics. Additionally, the diversity and quality of signals when the virtual dataset was used as the source dataset also influenced the transfer effect. In conclusion, transfer learning can effectively enhance the performance of DL models in cardiovascular signal processing, but the transfer effect may vary depending on the source datasets and transfer methods employed.

To compare the effects of CASP and CADP in the reconstructed CAP waveforms of the CBiMSA\_trans model on different transfer datasets, we utilized linear regression plots and Bland-Altman plots, focusing on the public databases Dataset\_PPG2RAP, Dataset\_HN, and the multiscale virtual database Dataset\_TLM for comparison. Fig. 5, Fig. 6 and Fig. 7 present the Bland-Altman plots and linear regression plots of the CBiMSA\_trans model on the three datasets. In the Bland-Altman plots, the x-axis represents the mean between the CASP or CADP reconstructed by the different models and the invasively measured CASP or CADP, while the y-axis indicates the difference between them. The solid red line denotes the upper and lower limits of the 95\% consistency bounds, and the gray dashed line represents the mean of the prediction errors. It is evident that the majority of the error values detected by the CBiMSA\_trans model for CASP and CADP lie within the 95\% consistency limit. The CBiMSA\_trans model exhibits strong performance across different datasets, with the best performance observed in Dataset\_TLM. Based on the visual analysis and metric results, the coefficient of determination r\textsuperscript{2}\textsubscript{CASP} of the CBiMSA\_trans model on Dataset\_TLM is 0.98, with an RMSE\textsubscript{CASP} of 2.30, and the coefficient of determination r\textsuperscript{2}\textsubscript{CASP} is 0.98, with an RMSE\textsubscript{CADP} of 1.51. These findings indicate that the CBiMSA\_trans model excels in reconstructing CASP and CADP in CAP waveforms and demonstrates strong performance across different datasets.

\subsection{Effect of Training Net Layers}

\begin{table*}[ht]
\caption{Comparison of transfer learning performance fine-tuning across different network layers}
\label{layers}
	\centering
	\renewcommand{\arraystretch}{1.5}
	\setlength{\tabcolsep}{5pt}
	\begin{tabular*}{\linewidth}{lcccccc}
	\toprule
	Weight\_Fine\_Tuning & MAE\textsubscript{CAP}$\downarrow$ & MAE\textsubscript{CASP}$\downarrow$ & RMSE\textsubscript{CASP}$\downarrow$ & MAE\textsubscript{CADP}$\downarrow$ & RMSE\textsubscript{CADP}$\downarrow$ & SCC\textsubscript{CAP}$\uparrow$ \\

    \midrule
    FC                    & 2.36±0.41  & 1.99±0.35 & 2.59±0.42 & 1.41±0.30 & 1.69±0.36 & 0.96   \\
    MSA, FC             & 2.80±0.86  & 2.37±0.65 & 3.09±0.80 & 1.35±0.24 & 1.67±0.29 & 0.96   \\
    BiLSTM, FC             & 2.59±0.50  & 2.25±0.51 & 2.95±0.68 & 1.43±0.27 & 1.78±0.34 & 0.96   \\
    BiLSTM, MSA, FC       & 2.31±0.36  & 1.88±0.40 & 2.50±0.53 & 1.26±0.23 & 1.57±0.25 & 0.96   \\
    Conv1, Conv2, FC        & 2.61±0.76  & 2.07±0.50 & 2.70±0.68 & 1.46±0.28 & 1.81±0.28 & 0.96   \\
    Conv1, Conv2, MSA, FC & 2.26±0.73  & 1.83±0.46 & 2.47±0.60 & 1.24±0.27 & 1.55±0.31 & 0.96   \\
    Conv1, Conv2, BiLSTM, MSA, FC   & \textbf{2.17±0. 21} & \textbf{1.77±0.23} & \textbf{2.30±0.32} & \textbf{1.22±0.26} & \textbf{1.51±0.30} & \textbf{0.96}   \\
    \bottomrule
    \end{tabular*}
\end{table*}

The transfer learning performance is compared in Table \ref{layers} by fine-tuning different sets of network layers with the target patient data. It is clear that retraining the Conv1-v2, BiLSTM, MSA, FC layers leads to the best transfer learning performance.

When fine-tuning all layers except BiLSTM, the MAE\textsubscript{CASP} and MAE\textsubscript{CADP} decreased to 2.26 mmHg and 1.22 mmHg, respectively. This can be attributed to the BiLSTM's ability to model temporal relationships between features, rather than the features themselves. This indicates that the temporal modeling of RAP features can be transferred across individuals, in addition to lower-level convolutional filters. Moreover, the retraining of the BiLSTM layer led to an improvement in both the average SBP and DBP MAE. This improvement may be attributed to the BiLSTM's capability to model the temporal relationship between features, underscoring the importance of temporal modeling of RAP features in addition to lower-level convolutional filters when transferring across individuals.

Fine-tuning all layers in a neural network is superior to adjusting a single layer or a fully connected layer individually. This approach offers several advantages, including enhanced performance and accuracy, the capacity to address challenging tasks such as low-resource or novel domains, and a reduction in training time and cost. By fine-tuning all layers, the model can leverage the generalization and regularization effects of the pretrained model, leading to improved efficiency and effectiveness. Furthermore, fine-tuning enables the reuse of a well-performing trained model, which is particularly beneficial when working with limited datasets. This method is particularly advantageous when the pretrained model is similar in size and domain to the new task, as it significantly reduces the effort required to adapt the model to the new task.

\section{Conculusion}
The proposed cross-domain and cross-modal transfer learning methods for CAP reconstruction offer a promising avenue for advancing cardiovascular health monitoring. By leveraging a novel cross-domain pretraining paradigm, which involves utilizing a virtual dataset generated by a hemodynamic simulation model, and a cross-modal transfer training approach that reconstructs from PPG signals to RAP signals, the study demonstrates the potential of integrating hemodynamic knowledge and learning waveform features to achieve high-precision CAP reconstruction.

The results indicate that the CBiMSA model, which integrates CNN, LSTM and a multi-head self-attention network, outperforms other models in both transfer and non-transfer scenarios. The model particularly excels in the cross-domain transfer learning based on the simulation dataset, showcasing its ability to effectively bridge the gap between hemodynamic simulation modeling and clinical physiological signal detection methods.

Overall, this research provides new potential value for further exploration and application in the field of cross-domain and cross-modal transfer of CAP reconstruction. The findings open up exciting opportunities for enhancing cardiovascular health monitoring through the integration of simulated and non-invasive physiological signal datasets, ultimately contributing to improved methods for assessing cardiovascular health and advancing clinical practice.

\section{Limitations}
Validation of DL models requires large databases. In this study, we use hemodynamic datasets that provide subjects for training and testing our models. The complete validation of our proposed CBiMSA\_trans model will require further studies using other clinical databases to further analyze and validate CAP estimates.

\section{PERSPECTIVES}
In this research, training and inference were carried out on a personal computer. Subsequent investigations will focus on developing a streamlined CAP estimation model suitable for direct implementation on a wearable device that can gather CAP data. This model is aimed at delivering performance levels similar to those achieved in our current study. Such an advancement will enable more immediate measurements, mitigate apprehensions regarding data transfer and privacy, and empower users to make timely adjustments for the promotion and sustenance of their well-being.

\begin{thebibliography}{00}

%% For numbered reference style
%% \bibitem{label}
%% Text of bibliographic item

\bibliographystyle{plain}%按作者姓氏排序
%上面两个二选一
\bibliography{reference}

% \bibitem{lamport94}
% Leslie Lamport,
% \textit{\LaTeX: a document preparation system},
% Addison Wesley, Massachusetts,
% 2nd edition,
% 1994.

\end{thebibliography}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
