用中文对代码进行注释，不改变原代码内容

# Langsmith：评估 LLM 应用能力的工具

Langsmith 是一种记录和评估通过 LangChain 构建的 LLM 应用的工具。它可以帮助我们更好地调整提示词等中间过程，从而优化应用效果。

## 构建 LangChain 和 LangSmith 优化链的示例代码

以下是使用 LangChain 和 LangSmith 优化链的示例代码：

- [如何使用 LangChain 和 LangSmith 优化链-CSDN博客](https://blog.csdn.net/wangjiansui/article/details/140329613)
- [LangSmith 教程 | Langchain v0.1](https://langchain114.com/docs/langsmith/walkthrough/)
- [Evaluation | 🦜️🛠️ LangSmith ~ 评估 | 🦜️🛠️ LangSmith (langchain.com)](https://docs.smith.langchain.com/concepts/evaluation#agents)

### 评估 LLM 应用

根据您或用户的重要标准来衡量应用程序的性能可能非常困难，但这是至关重要的。通过评估，您可以在迭代应用程序时使用一组固定的数据来衡量其性能。快速、可靠地获得这些洞察力，可以让您更加自信地进行改进。以下是一些评估步骤：

1. **创建用于衡量性能的初始黄金数据集**
2. **定义用于衡量性能的指标**
3. **对一些不同的提示或模型进行评估**
4. **手动比较结果**
5. **设置自动测试，以便在 CI/CD 中运行**

### 创建数据集

第一步是定义要评估的数据点：

- **模式**：每个数据点至少应包括应用程序的输入。如果可能，定义预期输出也非常有帮助。这代表您期望正常运行的应用程序输出内容。
- **数量**：没有硬性规定，但要确保适当覆盖可能需要防范的边缘案例。即使 10-50 个示例也能提供很大价值。
- **获取方法**：可以从手工收集最初的 10-20 个数据点开始，这些数据集一般会随着时间的推移而增长。

```python
from langsmith import Client

client = Client()

dataset_name = "QA Example Dataset"
try:
    dataset = client.create_dataset(dataset_name)
    client.create_examples(
        inputs=[
            {"question": "What is LangChain?"},
            {"question": "What is LangSmith?"},
            {"question": "What is OpenAI?"},
            {"question": "What is Google?"},
            {"question": "What is Mistral?"},
        ],
        outputs=[
            {"answer": "A framework for building LLM applications"},
            {"answer": "A platform for observing and evaluating LLM applications"},
            {"answer": "A company that creates Large Language Models"},
            {"answer": "A technology company known for search"},
            {"answer": "A company that creates Large Language Models"},
        ],
        dataset_id=dataset.id,
    )
except Exception as e:
    print(e)
```

创建后，可以在 LangSmith UI 中找到并查看 QA 示例数据集中的五个新示例。
![QA Example Dataset](https://docs.smith.langchain.com/assets/images/testing_tutorial_dataset-861cb88c2da002ca7122cf70e57ec3dd.png)

### 定义衡量标准

创建数据集后，可以定义一些指标来评估回答：

1. **正确性评估**：使用 LLM 来判断输出是否正确（相对于预期输出）。
2. **简洁度评估**：定义一个简单的 Python 函数来测量答案的长度。

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts.prompt import PromptTemplate
from langsmith.evaluation import LangChainStringEvaluator

_PROMPT_TEMPLATE = """You are an expert professor specialized in grading students' answers to questions.
You are grading the following question:
{query}
Here is the real answer:
{answer}
You are grading the following predicted answer:
{result}
Respond with CORRECT or INCORRECT:
Grade:
"""

PROMPT = PromptTemplate(
    input_variables=["query", "answer", "result"], template=_PROMPT_TEMPLATE
)

eval_llm = ChatOpenAI(
    model_name="glm-4",
    openai_api_base="https://open.bigmodel.cn/api/paas/v4",
)

qa_evaluator = LangChainStringEvaluator("qa", config={"llm": eval_llm, "prompt": PROMPT})
```

对于响应长度的评估：

```python
from langsmith.schemas import Run, Example

def evaluate_length(run: Run, example: Example) -> dict:
    prediction = run.outputs.get("output") or ""
    required = example.outputs.get("answer") or ""
    score = int(len(prediction) < 2 * len(required))
    return {"key":"length", "score": score}
```

### 运行评估

接下来，定义应用程序并运行评估：

```python
from langchain_core.messages import HumanMessage,SystemMessage

def my_app(question):
    message = [
        SystemMessage("Respond to the users question in a short, concise manner (one short sentence)."),
        HumanMessage(question),
    ]
    response = eval_llm.invoke(message)
    return response.content
```

定义封装器，将数据集的输入键映射到我们要调用的函数，然后将函数的输出映射到我们期望的输出键：

```python
def langsmith_app(inputs):
    output = my_app(inputs["question"])
    return {"output": output}
```

进行评估：

```python
from langsmith.evaluation import evaluate

experiment_results = evaluate(
    langsmith_app, # Your AI system
    data=dataset_name, # The data to predict and grade over
    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results
    experiment_prefix="GLM-4", # A prefix for your experiment names to easily identify them
)
```

### 在工作台查看和比较评估结果 ：

![[docs/01attachment/docs/Log/日志/2024-07-18-4/IMG-2024-07-31-15-52.png]]

### 设置自动化测试

```python
def test_length_score() -> None:
    """Test that the length score is at least 80%."""
    experiment_results = evaluate(
        langsmith_app, # Your AI system
        data=dataset_name, # The data to predict and grade over
        evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results
    )
    # This will be cleaned up in the next release:
    feedback = client.list_feedback(
        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],
        feedback_key="length"
    )
    scores = [f.score for f in feedback]
    assert sum(scores) / len(scores) >= 0.8, "Aggregate score should be at least .8"
```

## 评估 Agent

LangSmith 可以分别对 Agent 的最终响应、单个步骤和轨迹进行评估。

1. **评估最终响应**：输入是用户输入和（可选）工具列表，输出是 Agent 的最终响应。
2. **评估单个步骤**：输入是单个步骤的输入，输出是该步骤的输出，通常是 LLM 响应。
3. **评估轨迹**：输入是整个 Agent 的输入，输出是工具调用列表，可以是“精确”的轨迹或预期的工具调用列表。

这种评估方式有助于更好地理解 Agent 的行为并进行相应的优化。

### Evaluating an agent's final response 评估 Agent 的最终响应

[Evaluate an agent | 🦜️🛠️ LangSmith (langchain.com)](https://docs.smith.langchain.com/tutorials/Developers/agents#response-evaluation)

评估代理的一种方法是评估它在某项任务中的整体表现。这基本上就是把代理当作一个黑盒子，简单地评估它是否完成了任务。

输入应该是用户输入和（可选）工具列表。在某些情况下，工具被硬编码为代理的一部分，因此无需输入。在其他情况下，代理比较通用，这意味着它没有固定的工具集，因此需要在运行时输入工具。

输出应该是代理的最终响应。

评价器因要求代理执行的任务而异。许多代理会执行一系列相对复杂的步骤，并输出最终的文本响应。与 RAG 类似，LLM-as-judge 评价器在这些情况下通常也能有效地进行评价，因为它们可以直接从文本回复中评估代理是否完成了任务。

不过，这种评估方式有几个缺点。首先，它通常需要运行一段时间。其次，你并没有对代理内部发生的任何事情进行评估，因此在发生故障时很难进行调试。第三，有时很难定义适当的评估指标。

### Evaluating a single step of an agent 评估 Agent 的单个步骤

[Evaluate an agent | 🦜️🛠️ LangSmith (langchain.com)](https://docs.smith.langchain.com/tutorials/Developers/agents#single-step-evaluation)

代理通常会执行多个操作。虽然对它们进行端到端评估很有用，但对这些单个操作进行评估也很有用。这通常涉及评估代理的一个步骤，即代理决定要做什么的 LLM 调用。

输入应该是单个步骤的输入。根据测试内容的不同，这可能只是原始的用户输入（如提示和/或一组工具），也可能包括先前完成的步骤。

输出只是该步骤的输出，通常是 LLM 响应。LLM 响应通常包含工具调用，说明代理下一步应该采取什么行动。

其评估器通常是一些二进制分数，用于判断是否选择了正确的工具调用，以及一些启发式方法，用于判断对工具的输入是否正确。参考工具可以简单地指定为一个字符串。

这种评估方式有几个好处。它允许你对单个操作进行评估，从而找出应用程序可能失败的地方。运行速度也相对较快（因为只需调用一次 LLM），而且评估通常使用所选工具相对于参考工具的简单 heuristc 评估。它们的一个缺点是无法捕捉到整个代理过程，只能捕捉到一个特定的步骤。另一个缺点是数据集的创建具有挑战性，尤其是如果您想在代理输入中包含过去的历史记录。要为代理轨迹的早期步骤（例如，可能只包括输入提示）生成数据集非常容易，但要为轨迹的后期步骤（例如，包括代理之前的许多操作和响应）生成数据集却很困难。

### Evaluating an agent's trajectory 评估一个 Agent 的轨迹

[Evaluate an agent | 🦜️🛠️ LangSmith (langchain.com)](https://docs.smith.langchain.com/tutorials/Developers/agents#trajectory)

评估一个代理的轨迹包括查看代理所采取的所有步骤，并对步骤序列进行评估。

输入同样是整个代理的输入（用户输入和可选的工具列表）。

输出则是工具调用列表，可以是“精确”的轨迹（如预期的工具调用序列），也可以是预期的工具调用列表（顺序不限）。

这里的评估者是所采取步骤的某个函数。评估“精确”轨迹可以使用单个二进制分数，确认序列中每个工具名称的精确匹配。这种方法很简单，但也存在一些缺陷。有时可能存在多个正确路径。这种评估方法也无法捕捉到轨迹偏差一步与完全错误之间的区别。

为了解决这些缺陷，评价指标可以集中在“错误”步骤的数量上，这样就能更好地说明轨迹接近与偏离较大的区别。评价指标还可以侧重于是否以任何顺序调用了所有预期工具。

然而，这些方法都无法评估工具的输入；它们只关注所选择的工具。为了考虑到这一点，另一种评估技术是将完整的代理轨迹（连同参考轨迹）作为一组信息（例如所有 LLM 响应和工具调用）传递给 LLM 即判断。这可以评估代理的完整行为，但它是最难编译的参考（幸运的是，使用 LangGraph 这样的框架可以帮助解决这个问题！）。另一个缺点是，评估指标的提出可能有些棘手。

